<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="If the columns of $ A$ are linearly independent, then $Ax&#x3D;b$ has exactlyone solution for every $b$. It’s false. That is because there may be a case that $rank(A)&lt;m$ , whichmeans the vector $b$ coul">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2021/06/15/2020-11-05-linear%20algebra%201/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="If the columns of $ A$ are linearly independent, then $Ax&#x3D;b$ has exactlyone solution for every $b$. It’s false. That is because there may be a case that $rank(A)&lt;m$ , whichmeans the vector $b$ coul">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-06-15T01:11:30.151Z">
<meta property="article:modified_time" content="2021-06-14T09:39:48.000Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="page-2020-11-05-linear algebra 1" class="h-entry article article-type-page" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/06/15/2020-11-05-linear%20algebra%201/" class="article-date">
  <time class="dt-published" datetime="2021-06-15T01:11:30.151Z" itemprop="datePublished">2021-06-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Linear-Algebra/">Linear_Algebra</a>
  </div>

  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>If the columns of $ A$ are linearly independent, then $Ax=b$ has exactly<br>one solution for every $b$.</p>
<p>It’s false. That is because there may be a case that $rank(A)&lt;m$ , which<br>means the vector $b$ couldn’t be expressed by $A$.</p>
<p>Given $n$ vectors $a_i$ with $m$ components, what are the shapes of<br>$A,Q,R$ ?</p>
<p>The expression is $$A_{m\times n} = Q_{m\times n}R_{n\times n}$$</p>
<p>Suppose that $A=A_{m\times n}$, $B=B_{s\times t}$, $C=C_{s\times t}$ are<br>matrices, then $$rank \left[<br>    \begin{array}{cc}<br>    A &amp; O \<br>    C &amp; B \<br>    \end{array}<br>    \right]<br>    \geq rank(A)+rank(B)$$</p>
<p>It’s true. Because if the rank of $A$ equals $1$ , the rank of $B$<br>equals $2$, this satisfies the expression.</p>
<p>(Assume that $m=n=s=t=2$)</p>
<p>Give transformation to find the transform matrix.</p>
<p>We know that<br>$\mathscr{A}(\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n)=(\eta_1 \eta_2 \cdots \eta_n)$</p>
<p>Hence<br>$$A=(\eta_1 \eta_2 \cdots \eta_n)^{-1} (\mathscr{A}\varepsilon_1 \mathscr{A}\varepsilon_2 \cdots \mathscr{A}\varepsilon_n)$$<br>Therefore, we got $A$.</p>
<p>If $A$ is an $m$ by $n$ matrix and $rank(A)=n$, show that $A^{T}A$ is<br>invertible. Is $P=A(A^{T}A)^{-1}A^T $ invertible? Explain why.</p>
<p>Since $N(A^T A)=N(A), dimC(A^T)+dimN(A)=n$, also we have<br>$rankA=dimC(A^T)$,</p>
<p>Since $rankA=0,\Rightarrow dimN(A)=0, \Rightarrow dimN(A^T A)=0$,</p>
<p>Therefore, $A^T A$ is invertible.</p>
<p>Suppose $A$ is $m$ by $n$, $B$ is $n$ by $p$, and $AB = 0$ . Prove<br>$rankA+rankB\leq n$</p>
<p>We have $dimA=n$, and also $dim(C(A^T))+dim(N(A))=n, rankA=dimC(A^T)$<br>And $$AB=0, C(B)\subset N(A), \Rightarrow dimC(B)\leq dimN(A)$$</p>
<p>Therefore, we have $rankA+rankB\leq n$.</p>
<p>Show that an upper triangular matrix multipling another gives an upper<br>triangular matrix.</p>
<p>$$\begin{aligned}<br>\mathscr{A}(\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n)&amp;=(\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n)A\<br>\mathscr{A}(\eta_1 \eta_2 \cdots \eta_n)&amp;=(\eta_1 \eta_2 \cdots \eta_n)B\<br>(\eta_1 \eta_2 \cdots \eta_n)&amp;=(\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n)X\end{aligned}$$</p>
<p>It can be proved that: $$\begin{aligned}<br>\mathscr{A}(\eta_1 \eta_2 \cdots \eta_n)<br>&amp;=[\mathscr{A}(\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n)]X\&amp;=(\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n)AX \&amp;=(\eta_1 \eta_2 \cdots \eta_n)X^{-1}AX\end{aligned}$$<br>We get $B=X^{-1}AX$</p>
<p>Additionally, we add<br>$$\mathscr{A}([\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n]x)=\mathscr{A}(\alpha)=[\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n]Ax$$<br>$$\mathscr{A}(\alpha)=Ax$$<br>If we let<br>$[\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n]=I$, $A$ is<br>transforming effect. $x$ is the cofficient corrospond to the basis.<br>$[\varepsilon_1 \varepsilon_2 \cdots \varepsilon_n]$ is the basis before<br>transformation.</p>
<p>When we do some proves, it is easy to use the basis reprensenting all<br>the matrices or vectors in space, which means we have to introduce the<br>normal expression of them. Basis elements<br>$$A_{m\times n}=\sum_{i=1}^{m}\limits\sum_{j=1}^{n}\limits a_{ij}\alpha_i\beta_j  \Rightarrow A_{m\times n}=\sum_{i=1}^{m}\limits\sum_{j=1}^{n}\limits a_{ij} e_i e_j^T$$<br>After we choose the basis as identity basis. If $A=\pm A^T$, we have</p>
<p>$$A_{m\times n}=\sum_{i=1}^{m}\limits\sum_{j&gt;i}^{n}\limits a_{ij}( e_i e_j^T \pm e_j e_i^T)$$</p>
<p>The four possibilities for linear equations depend on the rank.</p>
<hr>
<p>   $r=m$   and   $r=n$   Square and invertible   $Ax=b$         $1$ solution<br>   $r=m$   and   $r&lt;n$      Short and wide       $Ax=b$      $\infty$ solutions<br>   $r&lt;m$   and   $r=n$       Tall and thin       $Ax=b$      $0$ or $1$ solution<br>   $r&lt;m$   and   $r&lt;n$       Not full rank       $Ax=b$   $0$ or $\infty$ solutions</p>
<hr>
<p>$ Ax=b $ is solvable if and only if $ y^T b=0 $ whenever $y^T A=0  $</p>
<p>$Ax=b$ is solvable if and only if $b\in C(A)$. Also $C(A)\perp N(A^T)$,<br>and $b\in N(A^T)$ if and only if $ y^T b=0 $ whenever $y^T A=0  $</p>
<p>$$P_Q(b)=Q(Q^TQ)^{-1}Q^T=(\sum\limits_{i=1}^{n} \frac{q_iq_i^T}{q_i^Tq_i})b$$</p>
<p>If eigenvectors $x_1, x_2, \cdots x_k$ correspond to different<br>eigenvalues $\lambda_1, \lambda_2 \cdots \lambda_k$ , then<br>$x_1, x_2, \cdots x_k$ is linearly independent.</p>
<p>Suppose $x_1, x_2, \cdots x_k$ is linearly dependent. Let $n$ be the<br>smallest positive integer such that $x_1, x_2, \cdots x_n$ is<br>independent.</p>
<p>$\exists a_1,a_2 \cdots a_n$ not all $0$, such that $$\begin{aligned}<br> a_1x_1+a_2x_2+\cdots a_nx_n=0\end{aligned}$$ Apply both sides<br>,$a_1\lambda_1 x_1+a_2 \lambda_2 x_2+\cdots +a_n\lambda_nx_n=0$, which<br>minus $\lambda_n \cdot (1)$<br>$$\Rightarrow a_1(\lambda_1-\lambda_n) x_1+a_2 (\lambda_2-\lambda_n) x_2+\cdots +a_n(\lambda_{n-1}-\lambda_n)x_n=0$$<br>$x_1 , x_2, \cdots x_n $ is independent<br>$\Rightarrow a_1=a_2=\cdots=a_{n-1}=0$.</p>
<p>Back to the (1), $a_nx_n=0\Rightarrow a_n=0$<br>$$\Rightarrow a_1=a_2=\cdots=a_{n-1}=a_n=0 \Rightarrow$$ A Contradiction$$</p>
<p>Different Expressions:<br>$$|A|=\sum\limits_{j=1}^{n}(-1)^{i+1}a_{ij}|M_{ij}|<br>=\sum_{j} (-1)^{\tau (j_1j_2\cdots j_n)} a_{1 j_1} a_{2 j_2} \cdots  a_{n j_n}$$<br>There is a big fomula:<br>$$|A|=\sum_{j} det(P)  a_{1 j_1} a_{2 j_2} \cdots  a_{n j_n}$$ Also we<br>have a relation:$\displaystyle A^{-1}=\frac{ C^T}{|A|}$$</p>
<p>For a difference equation, connected to eigenvalues:<br>$$A=Q\Lambda Q^T \Rightarrow A^k = Q\Lambda ^k Q^T$$</p>
<p>$p(x)=\frac{1}{2} x^T Ax-b^Tx$, so we<br>have$\nabla p_1(x)=Ax, \nabla p_2(x)=b.$ Therefore,<br>$$\nabla p(x)=Ax-b=0 \Rightarrow Ax=b$$</p>
<p>When $Ax=b$, for any $y\in {\mathbb{R}}$, we have<br>$$p(y)-p(x)=\frac{1}{2} y^T Ay-b^Ty-(\frac{1}{2} x^T Ax-b^Tx)=<br> \frac{1}{2} (y-x)^T A(y-x)\geqslant 0 $$ </p>
<p> if and only if $A$ is positive<br>definite.</p>
<p>$$L(x,y)=p(x)+y^T(Cx-d)=\frac{1}{2} x^T Ax-b^Tx+x^T C^Ty-y^T d,$$<br>and<br>$y=(y_1, y_2, \cdots y_n) \in {\mathbb{R}}$ is given vector(<em>Lagrange<br>multipliers</em>). So, it get the minimum value if and only if :<br>$$\begin{aligned}<br>\frac{\partial L}{\partial x}=0 : Ax+C^T y=b \qquad<br>\frac{\partial L}{\partial y}=0 : Cx=d\end{aligned}$$</p>
<p>Consider $\displaystyle R(x)=\frac{x^TAx}{x^Tx}$, and we will solve<br>$\min R(x)$. So we have :<br>$$\lambda_{\min}(A)\leqslant R(x)=\frac{x^TAx}{x^Tx}=\frac{(Qy)^TA(Qy)}{(Qy)^T(Qy)}=<br>    \frac{y^T \Lambda y}{y^Ty}\leqslant \lambda_{\max}(A)$$ Therefore,<br>we have<br>$$\lambda_{\min}(A)\leqslant R(x)={ x^TAx|  |x|=1 } \leqslant \lambda_{\max}(A)$$</p>
<p>$A_{m\times n}=U_{m\times m}\Sigma_{m\times n} V_{n\times n}^T$, and<br>$AV=U\Sigma$, is called SVD. We also have:<br>$$\begin{aligned}<br>A^TA=(V\Sigma^T U^T)U\Sigma V^T=V\Sigma^T \Sigma U &amp;\qquad<br>AA^T=(U\Sigma V^T)V\Sigma^T U^T=U\Sigma\Sigma^T U^T\<br>A[V_r|V_{n-r}]=[U_r|U_{n-r}]\Sigma&amp;= [\sigma_1u_1, \sigma_2u_2 \cdots \sigma_ru_r , 0 \cdots 0]\end{aligned}$$<br>and from this, we also have:<br>$$\begin{aligned}<br>V_{n-r} \rightarrow N(A) &amp;\qquad U_{m-r}\rightarrow N(A^T) \qquad<br>A^+ = U\Sigma^+ V^T\<br>V_r \rightarrow C(A^T)&amp;\qquad U_r \rightarrow C(A) \qquad x^+=A^+ b\end{aligned}$$</p>
<p>If $A_{s\times n}, B_{n\times m}$, show that<br>$rank(AB)\geqslant rank(A)+rank(B)-n$.</p>
<p>Since<br>$rank(A)+rank(B)=rank\begin{bmatrix}<br>    A &amp; O \<br>    O &amp; B<br>    \end{bmatrix}\geqslant rank\begin{bmatrix}<br>    A &amp; I \<br>    O &amp; B<br>    \end{bmatrix} $.<br>    Also we have elementary transformations:<br>$$\begin{bmatrix}<br>        A &amp; I \<br>        O &amp; B<br>    \end{bmatrix}\longrightarrow\begin{bmatrix}<br>    A &amp; I \<br>    -AB &amp; O<br>    \end{bmatrix}\longrightarrow\begin{bmatrix}<br>    O&amp; I \<br>    -AB &amp; O<br>    \end{bmatrix}$$ Therefore, $rank\begin{bmatrix}<br>    A &amp; I \<br>    O &amp; B<br>    \end{bmatrix}=rank\begin{bmatrix}<br>    O&amp; I \<br>    -AB &amp; O<br>    \end{bmatrix}=rank(AB)+rank(I)=rank(AB)+n$$</p>
<p>Let $R(A)=r_1, R(B)=r_2, R(AB)=r,$ We assume that<br>$b_1, b_2 \cdots b_{r_2}$ is the basic solutions of the column vectors<br>of $B$, so there must exists the largest number which satisfies<br>$Ab_j=0, j\in {1,2\cdots r_2}$ where $b_1, b_2 \cdots b_{r_2}$ is<br>$r_2-r$. Otherwise, it is contradictary to $R(AB)=r$.</p>
<p>It also means the $b_j$ of $Ab_j=0$ is in the $N(A)$, which means<br>$dim(N(A))=n-r_1$. Therefore, $r_2-r\leqslant n-r_1 \Rightarrow<br>    rank(AB)\geqslant rank(A)+rank(B)-n$.</p>
<h1 id="Linear-Calculation"><a href="#Linear-Calculation" class="headerlink" title="Linear Calculation"></a>Linear Calculation</h1><p>A $n$ by $n$ matrix multiplies an $n$ -dimensional vector and produces<br>an $m$ -dimensional vector.</p>
<p>$( E A$ times $x )$ equals $( E \operatorname { times } A x ) .$ We just<br>write $E A x$<br>$$A B = A \left[ \begin{array} { l } { b _ { 1 } } \ { b _ { 2 } } \ { b _ { 3 } } \end{array} \right] = \left[ \begin{array} { c } { A b _ { 1 } } \ { A b _ { 2 } } \ { A b _ { 3 } } \end{array} \right]$$<br>the number of columns in A has to equal the number of rows in $B .$ Then<br>$A$ can be multiplied into each column of $B .$</p>
<p>Each column of $A B$ is the product of a matrix and a column: column $j$<br>of $A B = A$ times (column $j$ of $B )$</p>
<p>Each row of $A B$ is the product of a row and a matrix: row $i$ of<br>$A B = ($ row $i$ of $A )$ times $B$</p>
<p>Matrix multiplication is associative: $( A B ) C = A ( B C ) .$ Just<br>write $A B C .$</p>
<p>Triangular factorization $A = L U$ with no exchanges of rows. $L$ is<br>lower triangular, with 1’s on the diagonal. The multipliers<br>$\ell _ { i j } ($ taken from elimination $)$ are below the diagonal.<br>$U$ is the upper triangular matrix which appeats after forward<br>elimination, The diagonal entries of $U$ are the pivots.</p>
<p>$$\left[ \begin{array} { c c c } { 1 } &amp; { 0 } &amp; { 0 } \ { \ell _ { 21 } } &amp; { 1 } &amp; { 0 } \ { \ell _ { 31 } } &amp; { \ell _ { 32 } } &amp; { 1 } \end{array} \right] \left[ \begin{array} { l l } { \text { row } 1 \text { of } U } \ { \text { row } 2 \text { of } U } \ { \text { row } 3 \text { of } U } \end{array} \right] =\mathrm{ original }A$$</p>
<p>$P A = L U$ $P ^ { - 1 }$ is always the same as<br>$P ^ { \mathrm { T } }$. With the rows reordered in advance, $P A$ can<br>be factored into $L U$</p>
<p>The inverse exists if and only if elimination produces n pivots (row<br>exchanges allowed). Elimination solves $A x = b$ without explicitly<br>finding $A ^ { - 1 } .$</p>
<p>Suppose there is a nonzero vector $x$ such that $A x = 0 .$ Then $A$<br>cannot have an inverse. To repeat: No matrix can bring 0 back to $x$ .<br>If $A$ is invertible, then $A x = 0$ can only have the zero solution<br>$x = 0$</p>
<p>$$\left[ \begin{array} { l l } { a } &amp; { b } \ { c } &amp; { d } \end{array} \right] ^ { - 1 } = \frac { 1 } { a d - b c } \left[ \begin{array} { c c } { d } &amp; { - b } \ { - c } &amp; { a } \end{array} \right]$$</p>
<p>$x = A ^ { - 1 } b$ separates into $L c = b $ and $ U x = c$ Invertible<br>$=$ Nonsingular ($n$ pivots)</p>
<p>The transpose of $A ^ { - 1 }$ is<br>$\left( A ^ { - 1 } \right) ^ { \mathrm { T } } = \left( A ^ { \mathrm { T } } \right) ^ { - 1 }$</p>
<p>Suppose $A = A ^ {T }$ can be factored into $A = L D U$ without row<br>exchanges. Then $U$ is the transpose of $L$ . The symmetric<br>factorization becomes $A = L D L ^ { T }$</p>
<p>The number of free variables consist of the basic solutions of $N(A)$.<br>The expression is normally:<br>$$N(A)= \left\lbrace x|x=k_1x_1+k_2x_2+k_3x_3 \right\rbrace$$</p>
<p>For a rectangular matrix, there dosen’t exist full inverse matrix.<br>However, there exist one-side matrix.</p>
<ol>
<li><p>Full row rank. $r=m\leqslant n$. There exists a right-side inverse<br> matrix.</p>
</li>
<li><p>Full column rank. $r=n\leqslant m$. There exists a left-side inverse<br> matrix.</p>
</li>
</ol>
<h1 id="Linear-Space"><a href="#Linear-Space" class="headerlink" title="Linear Space"></a>Linear Space</h1><p>A <em>Vector space</em> is a set $V$ along with an additiont on $V$ and a<br>scalar multiplication on $V$ such that the following properties hold:</p>
<ol>
<li><p><strong>commutativity</strong><br> $u+v=v+u$ for all $u,v \in V$ ;</p>
</li>
<li><p><strong>associativity</strong><br> $(u+v)+w=u+(v+w) $ and $(ab)v=a(bv)$ for all $u,v,w  \in V $ and all<br> $a,b \in \textbf{F}$;</p>
</li>
<li><p><strong>addictive identity</strong><br> there exists an element $0\in V$ such that $v+0=v$ for all $v\in V$;</p>
</li>
<li><p><strong>addicyive inverse</strong><br> for every $v\in V$ ,there exists $w \in V$ such that $v+w=0$;</p>
</li>
<li><p> <strong>multiplicative identity</strong> $1v=v$ for all $v \in V$</p>
</li>
<li><p><strong>distributive properties</strong> $a(u+v)=au+av$ and $(a+b)v=av+bv$ for<br> all $a,b\in \textbf{F}$ and all $u,v\in V$.</p>
</li>
</ol>
<p>A subset $U$ of $V$ is called a <em>subspace</em> of $V$ if $U$ is also a<br>vector space (using the same addition and scalar multiplication as on<br>$V$).</p>
<p>A <em>subspace</em> of a vector space is a nonempty subset that satisfies the<br>requirements for a vector space: Linear combinations stay in the<br>subspace.</p>
<p>The system $A x = b$ is solvable if and only if the vector $b$ can be<br>expressed as a combination of the columns of $A .$ Then $b$ is in the<br>column space. We can describe all combinations of the two columns<br>geometrically: $A x = b$ can be solved if and only if b lies in the<br>plane that is spanned by the two column vectors.</p>
<p>The nullspace of a matrix consists of all vectors $x$ such that<br>$A x = 0 .$ It is denoted by $N ( A ) .$ It is a subspace of<br>$\mathbf { R } ^ { n } ,$ just as the column space was a subspace of<br>$\mathbf { R } ^ { m } .$</p>
<p>$A x _ { p } = b$ and $A x _ { n } = 0 \quad$ produce<br>$\quad A \left( x _ { p } + x _ { n } \right) = b$</p>
<p>$$R x = \left[ \begin{array} { l l l l } { 1 } &amp; { 3 } &amp; { 0 } &amp; { - 1 } \ { 0 } &amp; { 0 } &amp; { 1 } &amp; { 1 } \ { 0 } &amp; { 0 } &amp; { 0 } &amp; { 0 } \end{array} \right] \left[ \begin{array} { l } { u } \ { v } \ { w } \ { y } \end{array} \right] = \left[ \begin{array} { l } { 0 } \ { 0 } \ { 0 } \end{array} \right]$$<br>The unknowns $u , v , w , y$ go into two groups. One group contains the<br>pivot variables, those that correspond to columns with pivots.</p>
<p>If $A x = 0$ has more unknowns than equations $( n &gt; m ) ,$ it has at<br>least one special solution: There are more solutions than the trivial<br>$x = 0$</p>
<p>$x _ { \mathrm{complete} } = x _ { \mathrm{particular} } + x _ {\mathrm{  nullspace }}$</p>
<p>The columns of A are independent exactly when $N ( A ) = {$ zero vector<br>$}$</p>
<h2 id="Basis"><a href="#Basis" class="headerlink" title="Basis"></a>Basis</h2><p>Suppose $c _ { 1 } v _ { 1 } + \cdots + c _ { k } v _ { k } = 0$ only<br>happens when $c _ { 1 } = \cdots = c _ { k } = 0 .$ Then the vectors<br>$v _ { 1 } , \ldots , v _ { k }$ are linearly independent. If any<br>$c ^ { \prime }$ are nonzero, the $v ^ { \prime } \mathrm { s }$ are<br>linearly dependent. One vector is a combination of the others.</p>
<p>To check any set of vectors $v _ { 1 } , \ldots , v _ { n }$ for<br>independence, put them in the columns of $A .$ Then solve the system<br>$A c = 0 ;$ the vectors are dependent if there is a solution other than<br>$c = 0 .$ With no free variables ( rank is $n$) , there is no nullspace<br>except $c = 0 ;$ the vectors are independent. If the rank is less than<br>$n ,$ at least one free variable can be nonzero and the columns are<br>dependent.</p>
<p>A set of n vectors in $\mathbf { R } ^ { m }$ must be linearly dependent<br>if $n &gt; m$</p>
<p>A basis for $\mathrm { V }$ is a sequence of vectors having two<br>properties at once:</p>
<ol>
<li><p> The vectors are linearly independent (not too many vectors).</p>
</li>
<li><p> They span the space V (not too few vectors).</p>
</li>
</ol>
<p>Any two bases for a vector space $\mathbf { V }$ contain the same number<br>of vec- tors. This number, which is shared by all bases and expresses<br>the number of $\cdot$ degrees of freedom” of the space, is the dimension<br>of $\mathbf { V } .$</p>
<p>If $v _ { 1 } , \ldots , v _ { m }$ and $w _ { 1 } , \ldots , w _ { n }$<br>are both bases for the same vector space, then $m = n .$ The number of<br>vectors is the same.</p>
<p>Suppose there are more $w ^ { \prime }$ s than<br>$v ^ { \prime } \mathrm { s } ( n &gt; m ) .$ We will arrive at a<br>contradiction. Since the $v ^ { \prime }$ s form a basis, they must span<br>the space. Every $w _ { j }$ can be written as a combination of the v’s:<br>If $w _ { 1 } = a _ { 11 } v _ { 1 } + \cdots + a _ { m 1 } v _ { m } ,$<br>this is the first column of a matrix multiplication $V A :$<br>$$W = \left[ \begin{array} { l l l l } { w _ { 1 } } &amp; { w _ { 2 } } &amp; { \cdots } &amp; { w _ { n } } \end{array} \right] = \left[ \begin{array} { c c c } { v _ { 1 } } &amp; { \cdots } &amp; { v _ { m } } \end{array} \right] \left[ \begin{array} { c } { a _ { 11 } } \ { \vdots } \ { a _ { m 1 } } \end{array} \right] = V A$$<br>We don’t know each $a _ { i j } ,$ but we know the shape of $A$ (it is<br>$m$ by $n ) .$ The second vector $w _ { 2 }$ is also a combination of<br>the $v ^ { \prime }$ . The coefficients in that combination fill the<br>second column of $A .$ The key is that $A$ has a row for every $v$ and a<br>column for every $w . A$ is a short, wide matrix, since $n &gt; m .$ There<br>is a nonzero solution to $A x = 0 .$ Then $VA x = 0$ which is $Wx = 0 .$<br>A combination of the $w$ ’s gives zero! The $w ^ { \prime }$ s could not<br>be a basis $-$ so we cannot have $n &gt; m$</p>
<p>If $m &gt; n$ we exchange the $v ^ { \prime }$ s and $w ^ { \prime }$ and<br>repeat the same steps. The only way to avoid a contradiction is to have<br>$m = n .$ This completes the proof that $m = n .$ To repeat: The<br>dimension of a space is the number of vectors in every basis.</p>
<p>The row space of $A$ has the same dimension $r$ as the row space of<br>$U ,$ and it has the same bases, because the row spaces of $A$ and $U ($<br>and $R )$ are the same.</p>
<p>The dimension of the column space $C ( A )$ equals the rank $r ,$ which<br>also equals the dimension of the row space: The number of independent<br>columns equals the number of independent rows. A basis for $C ( A )$ is<br>formed by the $r$ columns of $A$ that correspond, in $U ,$ to the<br>columns containing pivots.</p>
<p>Roughly speaking, an inverse exists only when the rank is as large as<br>possible.</p>
<h1 id="Important"><a href="#Important" class="headerlink" title="Important"></a>Important</h1><h2 id="Eigenvalues"><a href="#Eigenvalues" class="headerlink" title="Eigenvalues"></a>Eigenvalues</h2><p>Suppose the $n$ by $n$ matrix $A$ has $n$ linearly independent<br>eigenvectors. If these eigenvectors are the columns of a matrix $S ,$<br>then $S ^ { - 1 } A S$ is a diagonal matrix $\Lambda .$ The eigenvalues<br>of $A$ are on the diagonal of $\Lambda $</p>
<p><strong>Any matrix with distinct eigenvalues can be diagonalized.</strong></p>
<p>Not all matrices possess $n$ linearly independent eigenvectors, so not<br>all matrices are diagonalizable.</p>
<p>Diagonalizability of A depends on enough eigenvectors.<br>Invertibility of A depends on nonzero eigenvalues.<br>Diagonalization can fail only if there are repeated eigenvalues.</p>
<p>Diagonalizable matrices share the same eigenvector matrix $S$ if and<br>only if $A B = B A$</p>
<ol>
<li><p> Every symmetric matrix (and Hermitian matrix) has real eigenvalues.</p>
</li>
<li><p> Its eigenvectors can be chosen to be orthonormal.</p>
</li>
</ol>
<h2 id="complex"><a href="#complex" class="headerlink" title="complex"></a>complex</h2><p>A real symmetric matrix can be factored into<br>$A = Q \Lambda Q ^ { \mathrm { T } } .$ Its orthonormal eigenvectors are<br>in the orthogonal matrix $Q$ and its eigenvalues are in $\Lambda .$</p>
<p>A complex matrix with orthonormal columns is called a unitary matrix.</p>
<p>If $A$ is Hermitian then $K = i A$ is skew-Hermitian.</p>
<p>Suppose that $B = M ^ { - 1 } A M .$ Then $A$ and $B$ have the same<br>eigenvalues. Every eigenvector $x$ of $A$ corresponds to an eigenvector<br>$M ^ { - 1 } x$ of $B .$</p>
<p>The matrices $A$ and $B$ that represent the same linear transformation<br>$T$ with respect to two different bases (the $v ^ { \prime }$ s and the<br>$V ^ { \prime }$ s) are similar:</p>
<p>There is a unitary matrix $M = U$ such that $U ^ { - 1 } A U = T$ is<br>triangular. The eigenvalues of $A$ appear along the diagonal of this<br>similar matrix $T .$</p>
<p><strong>Spectral Theorem</strong> Every real symmetric $A$ can be diagonalized by an<br>orthogonal matrix $Q .$ Every Hermitian matrix can be diagonalized by a<br>unitary $U :$<br>$$\begin{aligned} Q ^ { - 1 } A Q = \Lambda \quad&amp; \text { or } \quad A = Q \Lambda Q ^ { \mathrm { T } } \ U ^ { - 1 } A U = \Lambda\quad &amp; \text { or } \quad A = U \Lambda U ^ { \mathrm { H } } \end{aligned}$$<br>The columns of $Q ($ or $U )$ contain orthonormal eigenvectors of $A$</p>
<h2 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h2><p>The matrix $N$ is normal if it commutes with<br>$$N N ^ { \mathrm { H } } = N ^ { \mathrm { H } } N .$$<br> For such matrices, and no others, the triangular $T = U ^ { - 1 } N U$ is the<br>diagonal $\Lambda$ Normal matrices are exactly those that have a<br>complete set of orthonormal eigenvectors.</p>
<ol>
<li><p>$A$ is diagonalizable: The columns of $S$ are eigenvectors and<br> $S ^ { - 1 } A S = \Lambda$</p>
</li>
<li><p>$A$ is arbitrary: The columns of M include “generalized<br> eigenvectors” of $A ,$ and the Jordan form $M ^ { - 1 } A M = J$ is<br> block diagonal.</p>
</li>
<li><p>$A$ is arbitrary: The unitary $U$ can be chosen so that<br> $U ^ { - 1 } A U = T$ is triangular.</p>
</li>
<li><p>$A$ is normal, $A A ^ { \mathrm { H } } = A ^ { \mathrm { H } } A :$<br> then $U$ can be chosen so that $U ^ { - 1 } A U = \Lambda$ <em>Special<br> cases of normal matrices, all with orthonormal eigenvectors:</em></p>
<ol>
<li><p>If $A = A ^ { H }$ is Hermitian, then all $\lambda _ { i }$ are<br> real.</p>
</li>
<li><p>If $A = A ^ { T }$ is real symmetric, then $\Lambda$ is real and<br> $U = Q$ is orthogonal.</p>
</li>
<li><p>If $A = - A ^ { H }$ is skew-Hermitian, then all<br> $\lambda _ { i }$ are purely imaginary.</p>
</li>
<li><p>If $A$ is orthogonal or unitary, then all<br> $\left| \lambda _ { i } \right| = 1$ are on the unit circle.</p>
</li>
</ol>
</li>
</ol>
<h1 id="Positive-definite"><a href="#Positive-definite" class="headerlink" title="Positive definite"></a>Positive definite</h1><p>$ a x ^ { 2 } + 2 b x y + c y ^ { 2 }$ is positive definite if and only<br>if $a &gt; 0$ and $a c &gt; b ^ { 2 } .$ Any $f ( x , y )$ has a minimum at a<br>point where $\partial F / \partial x = \partial F / \partial y = 0$ with</p>
<p>$$\frac { \partial ^ { 2 } F } { \partial x ^ { 2 } } &gt; 0 \quad \qquad \left[ \frac { \partial ^ { 2 }F  } { \partial x ^ { 2 } } \right] \left[ \frac { \partial  ^ { 2 }F } { \partial y ^ { 2 } } \right] &gt; \left[ \frac { \partial ^ { 2 } F } { \partial x \partial y } \right] ^ { 2 }$$</p>
<p>$$F ( x ) = F ( 0 ) + x ^ { \mathrm { T } } ( \text{ grad } F ) + \frac { 1 } { 2 } x ^ { \mathrm { T } } A x +\text{higher order terms}$$</p>
<p>At a stationary point,<br>$\nabla F = \left( \partial F / \partial x _ { 1 } , \ldots , \partial F / \partial x _ { n } \right)$<br>is a vector of zeros.</p>
<p>$$a x ^ { 2 } + 2 b x y + c y ^ { 2 } = a \left( x + \frac { b } { a } y \right) ^ { 2 } + \frac { a c - b ^ { 2 } } { a } y ^ { 2 }$$</p>
<p>Each of the following tests is a necessary and sufficient condition for<br>the real symmetric matrix $A$ to be positive definite:</p>
<ol>
<li><p> $x ^ { \mathrm { T } } k x &gt; 0$ for all nonzero real vectors $x$ .</p>
</li>
<li><p> All the eigenvalues of $A$ satisfy $\lambda _ { i } &gt; 0$</p>
</li>
<li><p>All the upper left submatrices $A _ { k }$ have positive<br> determinants.</p>
</li>
<li><p> All the pivots (without row exchanges) satisfy $d _ { k } &gt; 0 .$</p>
</li>
<li><p>There is a matrix $R$ with independent columns such that<br> $A = R ^ { \mathrm { T } } R .$</p>
</li>
</ol>
<p>The key is to recognize $x ^ { \mathrm { T } } A x$ as<br>$x ^ { \mathrm { T } } R ^ { \mathrm { T } } R x = ( R x ) ^ { \mathrm { T } } ( R x )$</p>
<p>symmetric matrix $A$ to be positive semidefinite:</p>
<ol>
<li><p>$x ^ { \mathrm { T } } A x \geq 0$ for all vectors $x$ (this defines<br> positive semidefinite)</p>
</li>
<li><p> All the eigenvalues of $A$ satisfy $\lambda _ { i } \geq 0$</p>
</li>
<li><p> No principal submatrices have negative determinants.</p>
</li>
<li><p> No pivots are negative.</p>
</li>
<li><p>There is a matrix $R ,$ possibly with dependent columns, such that<br> $A = R ^ { \mathrm { T } } R$</p>
</li>
</ol>
<p>$$5 u ^ { 2 } + 8 u v + v ^ { 2 } = \left( \frac { u } { \sqrt { 2 } } - \frac { v } { \sqrt { 2 } } \right) ^ { 2 } + 9 \left( \frac { u } { \sqrt { 2 } } + \frac { v } { \sqrt { 2 } } \right) ^ { 2 } = 1$$</p>
<p>Any ellipsoid $x ^ { \mathrm { T } } A x = 1$ can be simplified in the<br>same way. The key step is to diago- nalize<br>$A = Q \Lambda Q ^ { \mathrm { T } } .$ We straightened the picture by<br>rotating the axes. Algebraically, the change to $y = Q ^ {T} x$ produces<br>a sum of squares:<br>$$x ^ { \mathrm { T } } A x = \left( x ^ { \mathrm { T } } Q \right) \Lambda \left( Q ^ { \mathrm { T } } x \right) = y ^ { \mathrm { T } } \Lambda y = \lambda _ { 1 } y _ { 1 } ^ { 2 } + \cdots + \lambda _ { n } y _ { n } ^ { 2 } = 1$$</p>
<p>Suppose $A = Q \Lambda Q ^ { \mathrm { T } }$ with<br>$\lambda _ { i } &gt; 0 .$ Rotating $y = Q ^ { \mathrm { T } } x$<br>simplifies $x ^ { \mathrm { T } } A x = 1 :$<br>$$x ^ { \mathrm { T } } Q \Lambda Q ^ { \mathrm { T } } x = 1  \qquad y ^ { \mathrm { T } } \Lambda y = 1 ,\qquad \lambda _ { 1 } y _ { 1 } ^ { 2 } + \cdots + \lambda _ { n } y _ { n } ^ { 2 } = 1$$</p>
<p>Congruence transformation $ A \rightarrow C ^ { \mathrm { T } } A C$ for<br>some nonsingular $C .$</p>
<p>$C ^ {T } A C $ has the same number of positive eigenvalues, negative<br>eigenvalues, and zero eigenvalues as $A .$</p>
<p>For any symmetric matrix $A ,$ the signs of the pivots agree with the<br>signs of the eigenvalues. The eigenvalue matrix $\Lambda$ and the pivot<br>matrix $D$ have the same number of positive entries, negative entries,<br>and zero entries.</p>
<p>The graph of $$P ( x ) = \frac { 1 } { 2 } A x ^ { 2 } - b x$$ has zero<br>slope when $$\frac { d P } { d x } = A x - b = 0$$</p>
<p>If $A$ is symmetric positive definite, then<br>$$P ( x ) = \frac { 1 } { 2 } x ^ { \mathrm { T } } A x - x ^ { T } b$$<br>reaches its minimum at the point where $A x = b .$ At that point<br>$$P _ { \min } = - \frac { 1 } { 2 } b ^ { \mathrm { T } } A ^ { - 1 } b$$</p>
<p>$$P _ { \min } = \frac { 1 } { 2 } \left( A ^ { - 1 } b \right) ^ { \mathrm { T } } A \left( A ^ { - 1 } b \right) - \left( A ^ { - 1 } b \right) ^ { \mathrm { T } } b = - \frac { 1 } { 2 } b ^ { \mathrm { T } } A ^ { - 1 } b$$</p>
<p>$$L ( x , y ) = P ( x ) + y ^ { \mathrm { T } } ( C x - d ) = \frac { 1 } { 2 } x ^ { \mathrm { T } } A x - x ^ { \mathrm { T } } b + x ^ { \mathrm { T } } C ^ { \mathrm { T } } y - y ^ { \mathrm { T } } d$$</p>
<p>$$\quad P _ { C / \min } = P _ { \min } + \frac { 1 } { 2 } y ^ { \mathrm { T } } \left( C A ^ { - 1 } b - d \right) \geq P _ { \min }$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/06/15/2020-11-05-linear%20algebra%201/" data-id="ckpxcsx41000vm4uqcn9dhxoy" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/06/15/2020-11-04-%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%AC%AC%E4%B8%89%E7%AB%A0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          经济学第三章
        
      </div>
    </a>
  
  
    <a href="/2021/06/15/2020-11-05-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%81%93/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">学习之道</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Economics/">Economics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/English/">English</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Introduction/">Introduction</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear_Algebra</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine_Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading-Notes/">Reading Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading-Notes/">Reading_Notes</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/06/15/2020-11-04-%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%AC%AC%E4%B8%89%E7%AB%A0/">经济学第三章</a>
          </li>
        
          <li>
            <a href="/2021/06/15/2020-11-05-linear%20algebra%201/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/06/15/2020-11-05-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%81%93/">学习之道</a>
          </li>
        
          <li>
            <a href="/2021/06/15/2020-11-14-Writing1/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/06/15/2020-11-06-%E5%88%AB%E6%83%B3%E6%91%86%E8%84%B1%E4%B9%A6/">别想摆脱书——艾柯卡里埃尔对话录</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>